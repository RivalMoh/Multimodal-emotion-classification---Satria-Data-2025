{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ca22b7",
   "metadata": {},
   "source": [
    "# üéØ Embedding Extraction Pipeline for Specialist Models\n",
    "\n",
    "This notebook extracts embeddings from all trained specialist models (audio, video, text) to be used in the fusion model training.\n",
    "\n",
    "## üìã Overview:\n",
    "1. **Load Trained Models**: Load your trained audio, video, and text specialist models\n",
    "2. **Prepare Datasets**: Set up datasets for each modality \n",
    "3. **Extract Embeddings**: Use each specialist model as an encoder to extract features\n",
    "4. **Save Results**: Store embeddings and metadata for fusion training\n",
    "\n",
    "## üéØ Key Features:\n",
    "- **Multi-modal Support**: Handles audio, video, and text specialists\n",
    "- **Efficient Processing**: Batch processing with GPU acceleration\n",
    "- **Robust Storage**: Saves embeddings as .npy files with metadata CSV\n",
    "- **Fusion Ready**: Outputs are ready for fusion model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ab3e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 8 emotion classes: ['Anger', 'Fear', 'Joy', 'Neutral', 'Proud', 'Sadness', 'Surprise', 'Trust']\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Setup & Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your existing extraction utilities\n",
    "from extract_embedding import (\n",
    "    extract_embeddings, \n",
    "    extract_multimodal_embeddings,\n",
    "    video_collate_fn,\n",
    "    audio_collate_fn, \n",
    "    text_collate_fn\n",
    ")\n",
    "\n",
    "# Import dataset classes\n",
    "from datasets import AudioDataset, VideoDataset, TextDataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load label mapping\n",
    "with open(\"artifacts/label2idx.json\", \"r\") as f:\n",
    "    label2idx = json.load(f)\n",
    "    \n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "print(f\"Loaded {len(label2idx)} emotion classes: {list(label2idx.keys())}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'batch_size': 8,\n",
    "    'output_dir': 'artifacts/embeddings',\n",
    "    'save_raw_embeddings': True,\n",
    "    'create_manifests': True\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520c1c1",
   "metadata": {},
   "source": [
    "# üîß Setup & Configuration\n",
    "\n",
    "This section handles all the imports, device setup, and configuration needed for the embedding extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138ad891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Loading Audio Specialist Model...\n",
      "Available audio models: ['best_audio_cnn_fold0.pth', 'enhanced_mlp_audio_specialist.pth', 'mlp_audio_specialist.pth', 'wavlm_cls_fold0.pth', 'wavlm_enhanced_fold0.pth']\n",
      "Loading: specialists/audio/mlp_audio_specialist.pth\n",
      "Secure loading failed due to sklearn objects, trying unsafe loading...\n",
      "Model state dict found, reconstructing architecture...\n",
      "Found model info at: specialists/audio/mlp_model_info.json\n",
      "Reconstructing MLP model with:\n",
      "  Input dim: 106\n",
      "  Hidden dims: [512, 256, 128]\n",
      "  Num classes: 8\n",
      "  Dropout: 0.3\n",
      "  Architecture: Linear -> BatchNorm -> ReLU -> Dropout\n",
      "‚úÖ Successfully reconstructed and loaded model from: specialists/audio/mlp_audio_specialist.pth\n",
      "‚úÖ Audio specialist loaded from: specialists/audio/mlp_audio_specialist.pth\n"
     ]
    }
   ],
   "source": [
    "# üéµ Load Audio Specialist Model\n",
    "def load_audio_specialist():\n",
    "    \"\"\"Load trained audio specialist model and convert to encoder\"\"\"\n",
    "    print(\"üéµ Loading Audio Specialist Model...\")\n",
    "    \n",
    "    # Check available audio models\n",
    "    audio_models_dir = \"specialists/audio/\"\n",
    "    if not os.path.exists(audio_models_dir):\n",
    "        print(f\"‚ùå Audio models directory not found: {audio_models_dir}\")\n",
    "        return None, None\n",
    "        \n",
    "    audio_models = [f for f in os.listdir(audio_models_dir) if f.endswith('.pth')]\n",
    "    print(f\"Available audio models: {audio_models}\")\n",
    "    \n",
    "    # Load the MLP audio specialist model specifically\n",
    "    model_paths = [\n",
    "        \"specialists/audio/mlp_audio_specialist.pth\"\n",
    "    ]\n",
    "    \n",
    "    audio_model = None\n",
    "    model_info = None\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Loading: {model_path}\")\n",
    "                # First try with weights_only=True (secure)\n",
    "                try:\n",
    "                    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "                except Exception as e:\n",
    "                    if \"sklearn\" in str(e) or \"WeightsUnpickler\" in str(e):\n",
    "                        print(f\"Secure loading failed due to sklearn objects, trying unsafe loading...\")\n",
    "                        # For trusted models, we can use weights_only=False\n",
    "                        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                if 'model' in checkpoint:\n",
    "                    audio_model = checkpoint['model']\n",
    "                elif 'model_state_dict' in checkpoint:\n",
    "                    print(\"Model state dict found, reconstructing architecture...\")\n",
    "                    # Try to load model info to reconstruct\n",
    "                    info_path = \"specialists/audio/mlp_model_info.json\"\n",
    "                    \n",
    "                    if os.path.exists(info_path):\n",
    "                        print(f\"Found model info at: {info_path}\")\n",
    "                        reconstructed_model = reconstruct_mlp_audio_model(info_path)\n",
    "                        if reconstructed_model is not None:\n",
    "                            reconstructed_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                            audio_model = reconstructed_model\n",
    "                            model_info = model_path\n",
    "                            print(f\"‚úÖ Successfully reconstructed and loaded model from: {model_path}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(f\"No model info found at {info_path}, skipping...\")\n",
    "                        continue\n",
    "                else:\n",
    "                    audio_model = checkpoint\n",
    "                \n",
    "                model_info = model_path\n",
    "                print(f\"‚úÖ Successfully loaded model from: {model_path}\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if audio_model is None:\n",
    "        print(\"‚ùå No audio models could be loaded!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert to encoder by removing classifier head if needed\n",
    "    class AudioEncoder(nn.Module):\n",
    "        def __init__(self, base_model):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        def forward(self, x):\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                return self.base_model.extract_features(x)\n",
    "            elif hasattr(self.base_model, 'feature_extractor'):\n",
    "                # For models with separate feature extractor\n",
    "                return self.base_model.feature_extractor(x)\n",
    "            elif hasattr(self.base_model, 'network'):\n",
    "                # For MLP models, get features before final classifier\n",
    "                layers = list(self.base_model.network.children())[:-1]\n",
    "                feature_extractor = nn.Sequential(*layers)\n",
    "                return feature_extractor(x)\n",
    "            else:\n",
    "                # Get intermediate representation before final classification\n",
    "                # For other models, get the last hidden layer before classifier\n",
    "                if hasattr(self.base_model, 'classifier'):\n",
    "                    # Remove the final classifier layer\n",
    "                    layers = list(self.base_model.children())[:-1]\n",
    "                    feature_extractor = nn.Sequential(*layers)\n",
    "                    return feature_extractor(x)\n",
    "                else:\n",
    "                    return self.base_model(x)\n",
    "        \n",
    "        def extract_features(self, x):\n",
    "            return self.forward(x)\n",
    "    \n",
    "    audio_encoder = AudioEncoder(audio_model)\n",
    "    audio_encoder.to(device)\n",
    "    audio_encoder.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Audio specialist loaded from: {model_info}\")\n",
    "    return audio_encoder, model_info\n",
    "\n",
    "def reconstruct_mlp_audio_model(model_info_path):\n",
    "    \"\"\"Reconstruct MLP audio model from saved info\"\"\"\n",
    "    try:\n",
    "        with open(model_info_path, 'r') as f:\n",
    "            info = json.load(f)\n",
    "        \n",
    "        # Create MLP model architecture with BatchNorm (matches the saved model)\n",
    "        class MLPAudioSpecialist(nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
    "                super().__init__()\n",
    "                \n",
    "                layers = []\n",
    "                prev_dim = input_dim\n",
    "                \n",
    "                for i, hidden_dim in enumerate(hidden_dims):\n",
    "                    # Linear layer\n",
    "                    layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                    # BatchNorm layer\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                    # ReLU activation\n",
    "                    layers.append(nn.ReLU())\n",
    "                    # Dropout\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                    prev_dim = hidden_dim\n",
    "                \n",
    "                # Final classifier\n",
    "                layers.append(nn.Linear(prev_dim, num_classes))\n",
    "                \n",
    "                # Only create the network (as saved in the checkpoint)\n",
    "                self.network = nn.Sequential(*layers)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.network(x)\n",
    "            \n",
    "            def extract_features(self, x):\n",
    "                # Extract features by removing the final classifier\n",
    "                layers = list(self.network.children())[:-1]\n",
    "                feature_extractor = nn.Sequential(*layers)\n",
    "                return feature_extractor(x)\n",
    "        \n",
    "        # Get model parameters from info\n",
    "        input_dim = info.get('feature_dim', 106)\n",
    "        hidden_dims = info.get('hidden_dims', [512, 256, 128])\n",
    "        num_classes = info.get('num_classes', 8)\n",
    "        dropout = info.get('dropout', 0.3)\n",
    "        \n",
    "        print(f\"Reconstructing MLP model with:\")\n",
    "        print(f\"  Input dim: {input_dim}\")\n",
    "        print(f\"  Hidden dims: {hidden_dims}\")\n",
    "        print(f\"  Num classes: {num_classes}\")\n",
    "        print(f\"  Dropout: {dropout}\")\n",
    "        print(f\"  Architecture: Linear -> BatchNorm -> ReLU -> Dropout\")\n",
    "        \n",
    "        model = MLPAudioSpecialist(input_dim, hidden_dims, num_classes, dropout)\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reconstruct model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load audio model\n",
    "audio_model, audio_model_info = load_audio_specialist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5962c0",
   "metadata": {},
   "source": [
    "# ü§ñ Model Loading\n",
    "\n",
    "This section loads all the trained specialist models (Audio, Video, Text) and prepares them for embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c36252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Loading Video Specialist Model...\n",
      "Available video models: ['video_fold0_cpu.pth', 'video_fold0_gpu.pth']\n",
      "Loading: specialists/video/video_fold0_gpu.pth\n",
      "‚ö†Ô∏è Video model is state_dict, creating placeholder encoder...\n",
      "‚úÖ Video specialist loaded from: specialists/video/video_fold0_gpu.pth\n"
     ]
    }
   ],
   "source": [
    "# üé¨ CELL 3: Load Video Specialist Model\n",
    "def load_video_specialist():\n",
    "    \"\"\"Load trained video specialist model and convert to encoder\"\"\"\n",
    "    print(\"üé¨ Loading Video Specialist Model...\")\n",
    "    \n",
    "    # Check available video models\n",
    "    video_models_dir = \"specialists/video/\"\n",
    "    if os.path.exists(video_models_dir):\n",
    "        video_models = [f for f in os.listdir(video_models_dir) if f.endswith('.pth')]\n",
    "        print(f\"Available video models: {video_models}\")\n",
    "    else:\n",
    "        print(\"‚ùå Video models directory not found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Try to load video models\n",
    "    model_paths = [\n",
    "        \"specialists/video/video_fold0_gpu.pth\"\n",
    "    ]\n",
    "    \n",
    "    video_model = None\n",
    "    model_info = None\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Loading: {model_path}\")\n",
    "                video_model = torch.load(model_path, map_location='cpu')\n",
    "                model_info = model_path\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if video_model is None:\n",
    "        print(\"‚ùå No video models could be loaded!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert to encoder\n",
    "    class VideoEncoder(nn.Module):\n",
    "        def __init__(self, base_model):\n",
    "            super().__init__()\n",
    "            # If base_model is an OrderedDict (state_dict), we need to create a proper model\n",
    "            if isinstance(base_model, dict):\n",
    "                print(\"‚ö†Ô∏è Video model is state_dict, creating placeholder encoder...\")\n",
    "                # Create a simple identity encoder for now\n",
    "                self.base_model = nn.Identity()\n",
    "                self.is_placeholder = True\n",
    "            else:\n",
    "                self.base_model = base_model\n",
    "                self.is_placeholder = False\n",
    "            \n",
    "        def forward(self, x):\n",
    "            if self.is_placeholder:\n",
    "                # For placeholder, return flattened features\n",
    "                if len(x.shape) == 5:  # [batch, frames, channels, height, width]\n",
    "                    batch_size, num_frames, channels, height, width = x.shape\n",
    "                    # Global average pooling over spatial dimensions and then over frames\n",
    "                    x = x.mean(dim=[3, 4])  # Average over H, W -> [batch, frames, channels]\n",
    "                    x = x.mean(dim=1)       # Average over frames -> [batch, channels]\n",
    "                    return x\n",
    "                else:\n",
    "                    return x.mean(dim=[2, 3]) if len(x.shape) > 2 else x\n",
    "            else:\n",
    "                # Handle video input: [batch, frames, channels, height, width]\n",
    "                if len(x.shape) == 5:\n",
    "                    batch_size, num_frames, channels, height, width = x.shape\n",
    "                    # Reshape to process all frames\n",
    "                    x = x.view(batch_size * num_frames, channels, height, width)\n",
    "                    features = self.base_model(x)\n",
    "                    # Reshape back and pool over frames\n",
    "                    if len(features.shape) > 1:\n",
    "                        features = features.view(batch_size, num_frames, -1)\n",
    "                        features = features.mean(dim=1)  # Average pooling over frames\n",
    "                    else:\n",
    "                        features = features.view(batch_size, -1)\n",
    "                    return features\n",
    "                else:\n",
    "                    return self.base_model(x)\n",
    "        \n",
    "        def extract_features(self, x):\n",
    "            return self.forward(x)\n",
    "    \n",
    "    video_encoder = VideoEncoder(video_model)\n",
    "    video_encoder.to(device)\n",
    "    video_encoder.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Video specialist loaded from: {model_info}\")\n",
    "    return video_encoder, model_info\n",
    "\n",
    "# Load video model\n",
    "video_model, video_model_info = load_video_specialist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ef687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Loading Text Specialist Model...\n",
      "Loading: specialists/transcript/multilingual_emotion_fold0.pth\n",
      "Text model state dict found. Reconstructing BERT-based model...\n",
      "Model name: bert-base-multilingual-cased\n",
      "Number of classes: 8\n",
      "‚úÖ Successfully reconstructed and loaded BERT model from: specialists/transcript/multilingual_emotion_fold0.pth\n",
      "‚úÖ Text specialist loaded from: specialists/transcript/multilingual_emotion_fold0.pth\n"
     ]
    }
   ],
   "source": [
    "# üìù CELL 4: Load Text Specialist Model (if available)\n",
    "def load_text_specialist():\n",
    "    \"\"\"Load trained text specialist model and convert to encoder\"\"\"\n",
    "    print(\"üìù Loading Text Specialist Model...\")\n",
    "    \n",
    "    # Check for text models\n",
    "    text_model_paths = [\n",
    "        \"specialists/transcript/multilingual_emotion_fold0.pth\"\n",
    "    ]\n",
    "    \n",
    "    text_model = None\n",
    "    model_info = None\n",
    "    \n",
    "    for model_path in text_model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                print(f\"Loading: {model_path}\")\n",
    "                checkpoint = torch.load(model_path, map_location='cpu')\n",
    "                \n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    # Reconstruct the BERT-based text model\n",
    "                    print(\"Text model state dict found. Reconstructing BERT-based model...\")\n",
    "                    \n",
    "                    # Get model info from checkpoint\n",
    "                    model_name = checkpoint.get('model_name', 'bert-base-multilingual-cased')\n",
    "                    num_classes = checkpoint.get('num_classes', 8)\n",
    "                    \n",
    "                    print(f\"Model name: {model_name}\")\n",
    "                    print(f\"Number of classes: {num_classes}\")\n",
    "                    \n",
    "                    # Create BERT-based emotion classifier\n",
    "                    from transformers import AutoModel, AutoConfig\n",
    "                    \n",
    "                    class BERTEmotionClassifier(nn.Module):\n",
    "                        def __init__(self, model_name, num_classes):\n",
    "                            super().__init__()\n",
    "                            # Load BERT model\n",
    "                            self.bert = AutoModel.from_pretrained(model_name)\n",
    "                            \n",
    "                            # Get BERT hidden size\n",
    "                            bert_hidden_size = self.bert.config.hidden_size\n",
    "                            \n",
    "                            # Classifier head\n",
    "                            self.classifier = nn.Linear(bert_hidden_size, num_classes)\n",
    "                            \n",
    "                        def forward(self, input_ids, attention_mask=None):\n",
    "                            # Get BERT outputs\n",
    "                            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                            # Use [CLS] token representation\n",
    "                            cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "                            # Classify\n",
    "                            logits = self.classifier(cls_output)\n",
    "                            return logits\n",
    "                        \n",
    "                        def extract_features(self, input_ids, attention_mask=None):\n",
    "                            # Extract features without classification\n",
    "                            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                            return outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "                    \n",
    "                    # Create model and load weights\n",
    "                    try:\n",
    "                        reconstructed_model = BERTEmotionClassifier(model_name, num_classes)\n",
    "                        reconstructed_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        text_model = reconstructed_model\n",
    "                        model_info = model_path\n",
    "                        print(f\"‚úÖ Successfully reconstructed and loaded BERT model from: {model_path}\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to reconstruct model: {e}\")\n",
    "                        # Try with offline mode\n",
    "                        print(\"Trying to load BERT in offline mode...\")\n",
    "                        try:\n",
    "                            # Create a basic BERT-like structure\n",
    "                            class SimpleBERTClassifier(nn.Module):\n",
    "                                def __init__(self, hidden_size=768, num_classes=8):\n",
    "                                    super().__init__()\n",
    "                                    self.hidden_size = hidden_size\n",
    "                                    self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "                                    \n",
    "                                def extract_features(self, input_ids, attention_mask=None):\n",
    "                                    # Return dummy features with correct shape\n",
    "                                    batch_size = input_ids.size(0)\n",
    "                                    return torch.zeros(batch_size, self.hidden_size)\n",
    "                                    \n",
    "                                def forward(self, input_ids, attention_mask=None):\n",
    "                                    features = self.extract_features(input_ids, attention_mask)\n",
    "                                    return self.classifier(features)\n",
    "                            \n",
    "                            simple_model = SimpleBERTClassifier()\n",
    "                            # Only load classifier weights\n",
    "                            classifier_state = {\n",
    "                                'classifier.weight': checkpoint['model_state_dict']['classifier.weight'],\n",
    "                                'classifier.bias': checkpoint['model_state_dict']['classifier.bias']\n",
    "                            }\n",
    "                            simple_model.load_state_dict(classifier_state, strict=False)\n",
    "                            text_model = simple_model\n",
    "                            model_info = model_path\n",
    "                            print(f\"‚ö†Ô∏è Loaded simplified model (classifier only) from: {model_path}\")\n",
    "                            break\n",
    "                        except Exception as e2:\n",
    "                            print(f\"Failed to load simplified model: {e2}\")\n",
    "                            continue\n",
    "                        \n",
    "                else:\n",
    "                    text_model = checkpoint\n",
    "                    model_info = model_path\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if text_model is None:\n",
    "        print(\"‚ö†Ô∏è No text models found or loaded. Will use placeholder.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert to encoder\n",
    "    class TextEncoder(nn.Module):\n",
    "        def __init__(self, base_model):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        def forward(self, input_ids, attention_mask=None, temporal_features=None):\n",
    "            if hasattr(self.base_model, 'bert'):\n",
    "                # Extract BERT features before classification\n",
    "                bert_output = self.base_model.bert(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                cls_output = bert_output.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "                \n",
    "                # Add temporal features if available\n",
    "                if temporal_features is not None and hasattr(self.base_model, 'temporal_processor'):\n",
    "                    temporal_processed = self.base_model.temporal_processor(temporal_features)\n",
    "                    features = torch.cat([cls_output, temporal_processed], dim=1)\n",
    "                else:\n",
    "                    features = cls_output\n",
    "                    \n",
    "                return features\n",
    "            else:\n",
    "                return self.base_model(input_ids, attention_mask, temporal_features)\n",
    "        \n",
    "        def extract_features(self, input_ids, attention_mask=None, temporal_features=None):\n",
    "            return self.forward(input_ids, attention_mask, temporal_features)\n",
    "    \n",
    "    text_encoder = TextEncoder(text_model)\n",
    "    text_encoder.to(device)\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Text specialist loaded from: {model_info}\")\n",
    "    return text_encoder, model_info\n",
    "\n",
    "# Load text model\n",
    "text_model, text_model_info = load_text_specialist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be5aa4",
   "metadata": {},
   "source": [
    "# üìä Dataset Preparation\n",
    "\n",
    "This section prepares the datasets for each modality and validates the data before extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a46c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Preparing Datasets for Embedding Extraction...\n",
      "Using manifest: artifacts/train_manifest_fold0.csv\n",
      "Loaded manifest with 4271 samples\n",
      "Columns: ['video_id', 'window_idx', 'start', 'end', 'frame_indices', 'frames_path', 'audio_path', 'text_snippet', 'label', 'label_idx', 'fold', 'split', 'speech_ratio', 'has_face']\n",
      "üéµ Preparing audio dataset...\n",
      "‚úÖ Audio dataset ready: 2949 samples\n",
      "üé¨ Preparing video dataset...\n",
      "‚úÖ Video dataset ready: 2949 samples\n",
      "üìù Preparing text dataset...\n",
      "‚úÖ Text dataset ready: 2949 samples\n",
      "\n",
      "üìä Summary: 3 datasets prepared\n",
      "\n",
      "üîç Verifying dataset alignment...\n",
      "Dataset sizes: {'audio': 2949, 'video': 2949, 'text': 2949}\n",
      "‚úÖ All datasets aligned with 2949 samples each\n"
     ]
    }
   ],
   "source": [
    "# üìä CELL 5: Prepare Datasets for Embedding Extraction\n",
    "def prepare_datasets():\n",
    "    \"\"\"Prepare datasets for each modality\"\"\"\n",
    "    print(\"üìä Preparing Datasets for Embedding Extraction...\")\n",
    "    \n",
    "    datasets = {}\n",
    "    manifests = {}\n",
    "    \n",
    "    # Check available manifest files\n",
    "    manifest_files = [\n",
    "        \"artifacts/train_manifest_fold0.csv\",\n",
    "        \"artifacts/val_manifest_fold0.csv\", \n",
    "        \"train/train_df.csv\",\n",
    "        \"train/val_df.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Find the best manifest file\n",
    "    manifest_path = None\n",
    "    for path in manifest_files:\n",
    "        if os.path.exists(path):\n",
    "            manifest_path = path\n",
    "            print(f\"Using manifest: {manifest_path}\")\n",
    "            break\n",
    "    \n",
    "    if manifest_path is None:\n",
    "        print(\"‚ùå No manifest files found!\")\n",
    "        return {}, {}\n",
    "    \n",
    "    # Load manifest\n",
    "    df = pd.read_csv(manifest_path)\n",
    "    print(f\"Loaded manifest with {len(df)} samples\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Take a subset for faster testing (you can increase this)\n",
    "    max_samples = 20000  # Adjust this number\n",
    "    filtered_manifest_path = manifest_path\n",
    "    \n",
    "    if len(df) > max_samples:\n",
    "        print(f\"‚ö° Using {max_samples} samples for faster processing\")\n",
    "        \n",
    "        # Strategy 1: Try to get diverse samples from different videos\n",
    "        unique_videos = df['video_id'].unique()\n",
    "        print(f\"üìä Total unique videos: {len(unique_videos)}\")\n",
    "        \n",
    "        if len(unique_videos) >= max_samples // 10:  # If we have enough videos\n",
    "            # Sample videos first, then take samples from each video\n",
    "            samples_per_video = max_samples // len(unique_videos)\n",
    "            remaining_samples = max_samples % len(unique_videos)\n",
    "            \n",
    "            sampled_rows = []\n",
    "            for i, video_id in enumerate(unique_videos):\n",
    "                video_df = df[df['video_id'] == video_id]\n",
    "                n_samples = samples_per_video + (1 if i < remaining_samples else 0)\n",
    "                n_samples = min(n_samples, len(video_df))\n",
    "                \n",
    "                sampled_video_rows = video_df.head(n_samples)\n",
    "                sampled_rows.append(sampled_video_rows)\n",
    "            \n",
    "            df_filtered = pd.concat(sampled_rows, ignore_index=True)\n",
    "            print(f\"üìä Diverse sampling: {len(df_filtered)} samples from {df_filtered['video_id'].nunique()} videos\")\n",
    "        else:\n",
    "            # Fallback: just take first N samples\n",
    "            df_filtered = df.head(max_samples)\n",
    "            print(f\"üìä Sequential sampling: {len(df_filtered)} samples from {df_filtered['video_id'].nunique()} videos\")\n",
    "        \n",
    "        # Save filtered manifest temporarily\n",
    "        filtered_manifest_path = manifest_path.replace('.csv', f'_filtered_{max_samples}.csv')\n",
    "        df_filtered.to_csv(filtered_manifest_path, index=False)\n",
    "        print(f\"üìÑ Created filtered manifest: {filtered_manifest_path}\")\n",
    "        print(f\"üìä Video distribution in filtered data:\")\n",
    "        video_counts = df_filtered['video_id'].value_counts().head(10)\n",
    "        for video_id, count in video_counts.items():\n",
    "            print(f\"   Video {video_id}: {count} samples\")\n",
    "    \n",
    "    # Prepare audio dataset\n",
    "    if audio_model is not None:\n",
    "        print(\"üéµ Preparing audio dataset...\")\n",
    "        try:\n",
    "            audio_dataset = AudioDataset(\n",
    "                manifest_csv=filtered_manifest_path,  # Use filtered manifest\n",
    "                split=\"train\", \n",
    "                fold=0,\n",
    "                label2idx=label2idx\n",
    "            )\n",
    "            # No need to limit again since we already filtered the manifest\n",
    "            \n",
    "            datasets['audio'] = audio_dataset\n",
    "            manifests['audio'] = filtered_manifest_path  # Use filtered manifest path\n",
    "            print(f\"‚úÖ Audio dataset ready: {len(audio_dataset)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create audio dataset: {e}\")\n",
    "    \n",
    "    # Prepare video dataset\n",
    "    if video_model is not None:\n",
    "        print(\"üé¨ Preparing video dataset...\")\n",
    "        try:\n",
    "            video_dataset = VideoDataset(\n",
    "                manifest_csv=filtered_manifest_path,  # Use filtered manifest\n",
    "                split=\"train\",\n",
    "                fold=0, \n",
    "                label2idx=label2idx\n",
    "            )\n",
    "            # No need to limit again since we already filtered the manifest\n",
    "                \n",
    "            datasets['video'] = video_dataset\n",
    "            manifests['video'] = filtered_manifest_path  # Use filtered manifest path\n",
    "            print(f\"‚úÖ Video dataset ready: {len(video_dataset)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create video dataset: {e}\")\n",
    "    \n",
    "    # Prepare text dataset \n",
    "    if text_model is not None:\n",
    "        print(\"üìù Preparing text dataset...\")\n",
    "        try:\n",
    "            text_dataset = TextDataset(\n",
    "                manifest_csv=filtered_manifest_path,  # Use filtered manifest\n",
    "                split=\"train\",\n",
    "                fold=0,\n",
    "                label2idx=label2idx\n",
    "            )\n",
    "            # No need to limit again since we already filtered the manifest\n",
    "                \n",
    "            datasets['text'] = text_dataset\n",
    "            manifests['text'] = filtered_manifest_path  # Use filtered manifest path\n",
    "            print(f\"‚úÖ Text dataset ready: {len(text_dataset)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create text dataset: {e}\")\n",
    "    \n",
    "    return datasets, manifests\n",
    "\n",
    "# Prepare datasets\n",
    "datasets, manifests = prepare_datasets()\n",
    "print(f\"\\nüìä Summary: {len(datasets)} datasets prepared\")\n",
    "\n",
    "# Verify dataset alignment\n",
    "if len(datasets) > 1:\n",
    "    print(f\"\\nüîç Verifying dataset alignment...\")\n",
    "    dataset_sizes = {name: len(dataset) for name, dataset in datasets.items()}\n",
    "    print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "    \n",
    "    # Check if all datasets have the same size\n",
    "    sizes = list(dataset_sizes.values())\n",
    "    if len(set(sizes)) == 1:\n",
    "        print(f\"‚úÖ All datasets aligned with {sizes[0]} samples each\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Dataset size mismatch! This might cause fusion issues.\")\n",
    "        min_size = min(sizes)\n",
    "        print(f\"üîß Consider using {min_size} samples for all modalities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcec060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Fixed Audio Feature Extraction...\n",
      "Raw audio shape: torch.Size([16000])\n",
      "Extracted features shape: (106,)\n",
      "Feature range: [-524.842, 2296.201]\n",
      "Feature type: float32\n",
      "‚úÖ Fixed audio feature extraction ready!\n"
     ]
    }
   ],
   "source": [
    "# üéµ PROPER AUDIO FEATURE EXTRACTION\n",
    "def extract_audio_features_106(audio_waveform, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract 106-dimensional audio features from raw waveform\n",
    "    This mimics the features likely used to train your MLP model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import librosa\n",
    "        import numpy as np\n",
    "        \n",
    "        # Ensure audio is numpy array and flatten it\n",
    "        if torch.is_tensor(audio_waveform):\n",
    "            audio = audio_waveform.cpu().numpy()\n",
    "        else:\n",
    "            audio = audio_waveform\n",
    "            \n",
    "        # Flatten audio if multi-dimensional\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.flatten()\n",
    "        \n",
    "        # Convert to float32 and handle empty or very short audio\n",
    "        audio = audio.astype(np.float32)\n",
    "        \n",
    "        if len(audio) == 0:\n",
    "            print(\"‚ö†Ô∏è Empty audio, returning zero features\")\n",
    "            return np.zeros(106, dtype=np.float32)\n",
    "        \n",
    "        if len(audio) < sample_rate // 10:  # Less than 0.1 seconds\n",
    "            print(f\"‚ö†Ô∏è Very short audio ({len(audio)} samples), padding...\")\n",
    "            # Pad with zeros to minimum length\n",
    "            min_length = sample_rate // 10\n",
    "            padded_audio = np.zeros(min_length, dtype=np.float32)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            audio = padded_audio\n",
    "        \n",
    "        # Normalize audio safely\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        try:\n",
    "            # 1. MFCC Features (13 coefficients √ó 4 statistics = 52 features)\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "            for i in range(13):\n",
    "                mfcc_coeffs = mfccs[i]\n",
    "                features.extend([\n",
    "                    float(np.mean(mfcc_coeffs)), \n",
    "                    float(np.std(mfcc_coeffs)), \n",
    "                    float(np.min(mfcc_coeffs)), \n",
    "                    float(np.max(mfcc_coeffs))\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è MFCC extraction failed: {e}, using zeros\")\n",
    "            features.extend([0.0] * 52)\n",
    "        \n",
    "        try:\n",
    "            # 2. Spectral Features (6 features)\n",
    "            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sample_rate)[0]\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)[0]\n",
    "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sample_rate)[0]\n",
    "            \n",
    "            features.extend([\n",
    "                float(np.mean(spectral_centroids)), float(np.std(spectral_centroids)),\n",
    "                float(np.mean(spectral_rolloff)), float(np.std(spectral_rolloff)),\n",
    "                float(np.mean(spectral_bandwidth)), float(np.std(spectral_bandwidth))\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Spectral features failed: {e}, using zeros\")\n",
    "            features.extend([0.0] * 6)\n",
    "        \n",
    "        try:\n",
    "            # 3. Zero Crossing Rate (1 feature)\n",
    "            zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "            features.append(float(np.mean(zcr)))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ZCR failed: {e}, using zero\")\n",
    "            features.append(0.0)\n",
    "        \n",
    "        try:\n",
    "            # 4. Chroma Features (12 features)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "            features.extend([float(np.mean(chroma[i])) for i in range(12)])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Chroma features failed: {e}, using zeros\")\n",
    "            features.extend([0.0] * 12)\n",
    "        \n",
    "        try:\n",
    "            # 5. RMS Energy (1 feature)\n",
    "            rms = librosa.feature.rms(y=audio)\n",
    "            features.append(float(np.mean(rms)))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è RMS failed: {e}, using zero\")\n",
    "            features.append(0.0)\n",
    "        \n",
    "        try:\n",
    "            # 6. Tempo (1 feature)\n",
    "            tempo, _ = librosa.beat.beat_track(y=audio, sr=sample_rate)\n",
    "            features.append(float(tempo))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Tempo extraction failed: {e}, using default\")\n",
    "            features.append(120.0)  # Default tempo\n",
    "        \n",
    "        try:\n",
    "            # 7. Additional spectral features to reach 106\n",
    "            mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "            features.extend([\n",
    "                float(np.mean(mel_spectrogram)), \n",
    "                float(np.std(mel_spectrogram)),\n",
    "                float(np.min(mel_spectrogram)), \n",
    "                float(np.max(mel_spectrogram))\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Mel spectrogram failed: {e}, using zeros\")\n",
    "            features.extend([0.0] * 4)\n",
    "        \n",
    "        # Ensure we have exactly 106 features\n",
    "        while len(features) < 106:\n",
    "            features.append(0.0)\n",
    "        features = features[:106]\n",
    "        \n",
    "        # Convert all to float32 and ensure no NaN/inf values\n",
    "        features = np.array(features, dtype=np.float32)\n",
    "        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è librosa not installed. Using simple features instead.\")\n",
    "        return extract_simple_audio_features(audio_waveform)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting audio features: {e}\")\n",
    "        return extract_simple_audio_features(audio_waveform)\n",
    "\n",
    "def extract_simple_audio_features(audio_waveform):\n",
    "    \"\"\"\n",
    "    Fallback: Simple statistical features when librosa is not available\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(audio_waveform):\n",
    "        audio = audio_waveform.cpu().numpy()\n",
    "    else:\n",
    "        audio = audio_waveform\n",
    "    \n",
    "    # Flatten if needed\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.flatten()\n",
    "    \n",
    "    # Convert to float32\n",
    "    audio = audio.astype(np.float32)\n",
    "    \n",
    "    if len(audio) == 0:\n",
    "        return np.zeros(106, dtype=np.float32)\n",
    "    \n",
    "    # Basic statistical features\n",
    "    features = [\n",
    "        float(np.mean(audio)), float(np.std(audio)), \n",
    "        float(np.min(audio)), float(np.max(audio)),\n",
    "        float(np.median(audio)), \n",
    "        float(np.percentile(audio, 25)), float(np.percentile(audio, 75)),\n",
    "        float(np.sum(audio > 0) / len(audio)),  # positive ratio\n",
    "        float(np.sum(np.abs(audio) > 0.1) / len(audio)),  # above threshold ratio\n",
    "    ]\n",
    "    \n",
    "    # Pad to 106 features\n",
    "    while len(features) < 106:\n",
    "        features.append(0.0)\n",
    "    \n",
    "    return np.array(features[:106], dtype=np.float32)\n",
    "\n",
    "# Test the audio feature extraction\n",
    "print(\"üß™ Testing Fixed Audio Feature Extraction...\")\n",
    "if 'audio' in datasets and len(datasets['audio']) > 0:\n",
    "    test_audio, _, _ = datasets['audio'][0]\n",
    "    print(f\"Raw audio shape: {test_audio.shape}\")\n",
    "    \n",
    "    # Test feature extraction\n",
    "    features = extract_audio_features_106(test_audio)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    print(f\"Feature range: [{features.min():.3f}, {features.max():.3f}]\")\n",
    "    print(f\"Feature type: {features.dtype}\")\n",
    "    print(\"‚úÖ Fixed audio feature extraction ready!\")\n",
    "else:\n",
    "    print(\"‚ùå No audio dataset available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c3f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing optimized extraction with small batch...\n",
      "üéØ OPTIMIZED EXTRACTION: Processing 50 samples\n",
      "üìä Available samples per modality: {'audio': 2949, 'video': 2949, 'text': 2949}\n",
      "üöÄ Processing all modalities for each sample together...\n",
      "============================================================\n",
      "‚ö° Starting processing... Sample 1/50\n",
      "‚ö° Progress: 2/50 (4.0%) | Time: 0.4s elapsed, 8.8s remaining | Speed: 5.5 samples/sec\n",
      "‚ö° Progress: 11/50 (22.0%) | Time: 0.7s elapsed, 2.3s remaining | Speed: 16.8 samples/sec\n",
      "\n",
      "üéØ === OPTIMIZED EXTRACTION COMPLETED ===\n",
      "‚è±Ô∏è  Total time: 0.0 minutes (1.6 seconds)\n",
      "‚ö° Average speed: 31.5 samples/second\n",
      "\n",
      "üìä AUDIO: 50 success, 0 errors (100.0% success)\n",
      "üìä VIDEO: 50 success, 0 errors (100.0% success)\n",
      "üìä TEXT: 50 success, 0 errors (100.0% success)\n"
     ]
    }
   ],
   "source": [
    "# üîß OPTIMIZED EMBEDDING EXTRACTION - FIXED VERSION\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_all_embeddings_optimized(max_samples=None, save_embeddings=True):\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Extract embeddings from all three modalities efficiently\n",
    "    Processes all modalities for the same sample together, avoiding redundancy\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'audio': {'success': 0, 'errors': 0, 'error_details': []},\n",
    "        'video': {'success': 0, 'errors': 0, 'error_details': []},\n",
    "        'text': {'success': 0, 'errors': 0, 'error_details': []}\n",
    "    }\n",
    "    \n",
    "    if max_samples is None:\n",
    "        max_samples = 20000\n",
    "    \n",
    "    # Determine the actual number of samples to process\n",
    "    available_samples = {}\n",
    "    for modality in ['audio', 'video', 'text']:\n",
    "        if modality in datasets:\n",
    "            available_samples[modality] = len(datasets[modality])\n",
    "        else:\n",
    "            available_samples[modality] = 0\n",
    "    \n",
    "    # Use the minimum available samples across all modalities\n",
    "    min_samples = min(available_samples.values()) if available_samples else 0\n",
    "    actual_samples = min(min_samples, max_samples)\n",
    "    \n",
    "    print(f\"üéØ OPTIMIZED EXTRACTION: Processing {actual_samples:,} samples\")\n",
    "    print(f\"üìä Available samples per modality: {available_samples}\")\n",
    "    print(f\"üöÄ Processing all modalities for each sample together...\")\n",
    "    print(\"=\" * 60)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process samples efficiently - all modalities per sample\n",
    "    for i in range(actual_samples):\n",
    "        sample_start_time = time.time()\n",
    "        \n",
    "        # Audio Processing\n",
    "        if 'audio' in datasets and audio_model is not None:\n",
    "            try:\n",
    "                audio_waveform, label, video_id = datasets['audio'][i]\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_audio_features_106(audio_waveform)\n",
    "                features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(audio_model, 'cuda'):\n",
    "                        features_tensor = features_tensor.cuda()\n",
    "                    \n",
    "                    audio_embedding = audio_model(features_tensor)\n",
    "                    if audio_embedding.dim() > 1:\n",
    "                        audio_embedding = audio_embedding.squeeze()\n",
    "                    audio_embedding = audio_embedding.cpu().numpy()\n",
    "                \n",
    "                # Validate and save\n",
    "                if not (np.any(np.isnan(audio_embedding)) or np.any(np.isinf(audio_embedding))):\n",
    "                    if save_embeddings:\n",
    "                        audio_save_path = f\"specialists/audio/audio_embedding_{i:06d}.npy\"\n",
    "                        np.save(audio_save_path, audio_embedding)\n",
    "                    results['audio']['success'] += 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid embedding values\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['audio']['errors'] += 1\n",
    "                if len(results['audio']['error_details']) < 5:\n",
    "                    results['audio']['error_details'].append(f\"Sample {i}: {str(e)}\")\n",
    "                    print(f\"‚ùå Audio error {i}: {e}\")\n",
    "                    sys.stdout.flush()\n",
    "        \n",
    "        # Video Processing\n",
    "        if 'video' in datasets and video_model is not None:\n",
    "            try:\n",
    "                frames, label, video_id = datasets['video'][i]\n",
    "                \n",
    "                # Format frames\n",
    "                if isinstance(frames, list):\n",
    "                    frames = torch.stack(frames)\n",
    "                if frames.dim() == 4:\n",
    "                    frames = frames.unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(video_model, 'cuda'):\n",
    "                        frames = frames.cuda()\n",
    "                    \n",
    "                    video_embedding = video_model(frames)\n",
    "                    if video_embedding.dim() > 1:\n",
    "                        video_embedding = video_embedding.squeeze()\n",
    "                    video_embedding = video_embedding.cpu().numpy()\n",
    "                \n",
    "                # Validate and save\n",
    "                if not (np.any(np.isnan(video_embedding)) or np.any(np.isinf(video_embedding))):\n",
    "                    if save_embeddings:\n",
    "                        video_save_path = f\"specialists/video/video_embedding_{i:06d}.npy\"\n",
    "                        np.save(video_save_path, video_embedding)\n",
    "                    results['video']['success'] += 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid embedding values\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['video']['errors'] += 1\n",
    "                if len(results['video']['error_details']) < 5:\n",
    "                    results['video']['error_details'].append(f\"Sample {i}: {str(e)}\")\n",
    "                    print(f\"‚ùå Video error {i}: {e}\")\n",
    "                    sys.stdout.flush()\n",
    "        \n",
    "        # Text Processing\n",
    "        if 'text' in datasets and text_model is not None:\n",
    "            try:\n",
    "                sample = datasets['text'][i]\n",
    "                \n",
    "                if len(sample) == 3:\n",
    "                    tokenized, label, metadata = sample\n",
    "                    input_ids = torch.tensor(tokenized['input_ids'])\n",
    "                    attention_mask = torch.tensor(tokenized['attention_mask'])\n",
    "                elif len(sample) == 4:\n",
    "                    input_ids, attention_mask, label, video_id = sample\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected text sample format: {len(sample)} elements\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if hasattr(text_model, 'cuda'):\n",
    "                        input_ids = input_ids.cuda()\n",
    "                        attention_mask = attention_mask.cuda()\n",
    "                    \n",
    "                    text_embedding = text_model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "                    if text_embedding.dim() > 1:\n",
    "                        text_embedding = text_embedding.squeeze()\n",
    "                    text_embedding = text_embedding.cpu().numpy()\n",
    "                \n",
    "                # Validate and save\n",
    "                if not (np.any(np.isnan(text_embedding)) or np.any(np.isinf(text_embedding))):\n",
    "                    if save_embeddings:\n",
    "                        text_save_path = f\"specialists/text/text_embedding_{i:06d}.npy\"\n",
    "                        np.save(text_save_path, text_embedding)\n",
    "                    results['text']['success'] += 1\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid embedding values\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results['text']['errors'] += 1\n",
    "                if len(results['text']['error_details']) < 5:\n",
    "                    results['text']['error_details'].append(f\"Sample {i}: {str(e)}\")\n",
    "                    print(f\"‚ùå Text error {i}: {e}\")\n",
    "                    sys.stdout.flush()\n",
    "        \n",
    "        # Progress reporting with time estimates\n",
    "        if i % 100 == 0 or i in [1, 10, 50]:  # More frequent progress updates\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if i > 0:\n",
    "                avg_time_per_sample = elapsed_time / (i + 1)\n",
    "                remaining_samples = actual_samples - (i + 1)\n",
    "                estimated_remaining_time = avg_time_per_sample * remaining_samples\n",
    "                \n",
    "                # Format time estimates\n",
    "                def format_time(seconds):\n",
    "                    if seconds < 60:\n",
    "                        return f\"{seconds:.1f}s\"\n",
    "                    elif seconds < 3600:\n",
    "                        return f\"{seconds/60:.1f}m\"\n",
    "                    else:\n",
    "                        return f\"{seconds/3600:.1f}h\"\n",
    "                \n",
    "                print(f\"‚ö° Progress: {i+1:,}/{actual_samples:,} ({(i+1)/actual_samples*100:.1f}%) | \"\n",
    "                      f\"Time: {format_time(elapsed_time)} elapsed, {format_time(estimated_remaining_time)} remaining | \"\n",
    "                      f\"Speed: {(i+1)/elapsed_time:.1f} samples/sec\")\n",
    "            else:\n",
    "                print(f\"‚ö° Starting processing... Sample {i+1:,}/{actual_samples:,}\")\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nüéØ === OPTIMIZED EXTRACTION COMPLETED ===\")\n",
    "    print(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes ({total_time:.1f} seconds)\")\n",
    "    print(f\"‚ö° Average speed: {actual_samples/total_time:.1f} samples/second\")\n",
    "    print()\n",
    "    \n",
    "    for modality, result in results.items():\n",
    "        total_processed = result['success'] + result['errors']\n",
    "        success_rate = (result['success'] / total_processed * 100) if total_processed > 0 else 0\n",
    "        print(f\"üìä {modality.upper()}: {result['success']:,} success, {result['errors']:,} errors ({success_rate:.1f}% success)\")\n",
    "        \n",
    "        if result['error_details']:\n",
    "            print(f\"   Sample errors: {result['error_details'][:3]}\")\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    return results\n",
    "\n",
    "# Test the optimized version with a small batch first\n",
    "print(\"üß™ Testing optimized extraction with small batch...\")\n",
    "sys.stdout.flush()\n",
    "test_results_optimized = extract_all_embeddings_optimized(max_samples=50, save_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b6203",
   "metadata": {},
   "source": [
    "# üöÄ Embedding Extraction\n",
    "\n",
    "This section contains the main embedding extraction pipeline with all necessary functions and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad39027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STARTING OPTIMIZED EMBEDDING EXTRACTION PIPELINE\n",
      "üïê Started at: 2025-09-20 13:12:29\n",
      "============================================================\n",
      "\n",
      "üìÅ Step 1: Creating directories...\n",
      "‚úÖ Directories created\n",
      "\n",
      "üöÄ Step 2: Extracting embeddings with optimized pipeline...\n",
      "‚ö° This processes all modalities together for maximum efficiency\n",
      "üéØ OPTIMIZED EXTRACTION: Processing 2,949 samples\n",
      "üìä Available samples per modality: {'audio': 2949, 'video': 2949, 'text': 2949}\n",
      "üöÄ Processing all modalities for each sample together...\n",
      "============================================================\n",
      "‚ö° Starting processing... Sample 1/2,949\n",
      "‚ö° Progress: 2/2,949 (0.1%) | Time: 0.1s elapsed, 2.0m remaining | Speed: 24.3 samples/sec\n",
      "‚ö° Progress: 11/2,949 (0.4%) | Time: 0.4s elapsed, 1.6m remaining | Speed: 30.2 samples/sec\n",
      "‚ö° Progress: 51/2,949 (1.7%) | Time: 1.6s elapsed, 1.5m remaining | Speed: 32.6 samples/sec\n",
      "‚ö†Ô∏è Very short audio (19 samples), padding...\n",
      "‚ö° Progress: 101/2,949 (3.4%) | Time: 4.7s elapsed, 2.2m remaining | Speed: 21.6 samples/sec\n",
      "‚ö° Progress: 201/2,949 (6.8%) | Time: 13.0s elapsed, 3.0m remaining | Speed: 15.5 samples/sec\n",
      "‚ö° Progress: 301/2,949 (10.2%) | Time: 22.4s elapsed, 3.3m remaining | Speed: 13.4 samples/sec\n",
      "‚ö° Progress: 401/2,949 (13.6%) | Time: 30.5s elapsed, 3.2m remaining | Speed: 13.1 samples/sec\n",
      "‚ö†Ô∏è Very short audio (480 samples), padding...\n",
      "‚ö° Progress: 501/2,949 (17.0%) | Time: 39.1s elapsed, 3.2m remaining | Speed: 12.8 samples/sec\n",
      "‚ö° Progress: 601/2,949 (20.4%) | Time: 46.9s elapsed, 3.1m remaining | Speed: 12.8 samples/sec\n",
      "‚ö° Progress: 701/2,949 (23.8%) | Time: 54.0s elapsed, 2.9m remaining | Speed: 13.0 samples/sec\n",
      "‚ö° Progress: 801/2,949 (27.2%) | Time: 1.0m elapsed, 2.8m remaining | Speed: 12.8 samples/sec\n",
      "‚ö° Progress: 901/2,949 (30.6%) | Time: 1.2m elapsed, 2.7m remaining | Speed: 12.7 samples/sec\n",
      "‚ö° Progress: 1,001/2,949 (33.9%) | Time: 1.3m elapsed, 2.6m remaining | Speed: 12.6 samples/sec\n",
      "‚ö° Progress: 1,101/2,949 (37.3%) | Time: 1.5m elapsed, 2.5m remaining | Speed: 12.5 samples/sec\n",
      "‚ö° Progress: 1,201/2,949 (40.7%) | Time: 1.6m elapsed, 2.3m remaining | Speed: 12.5 samples/sec\n",
      "‚ö° Progress: 1,301/2,949 (44.1%) | Time: 1.7m elapsed, 2.2m remaining | Speed: 12.4 samples/sec\n",
      "‚ö†Ô∏è Very short audio (1280 samples), padding...\n",
      "‚ö° Progress: 1,401/2,949 (47.5%) | Time: 1.9m elapsed, 2.0m remaining | Speed: 12.6 samples/sec\n",
      "‚ö° Progress: 1,501/2,949 (50.9%) | Time: 1.9m elapsed, 1.9m remaining | Speed: 12.9 samples/sec\n",
      "‚ö†Ô∏è Very short audio (1067 samples), padding...\n",
      "‚ö° Progress: 1,601/2,949 (54.3%) | Time: 2.1m elapsed, 1.8m remaining | Speed: 12.8 samples/sec\n",
      "‚ö° Progress: 1,701/2,949 (57.7%) | Time: 2.2m elapsed, 1.6m remaining | Speed: 12.7 samples/sec\n",
      "‚ö†Ô∏è Very short audio (1280 samples), padding...\n",
      "‚ö° Progress: 1,801/2,949 (61.1%) | Time: 2.4m elapsed, 1.5m remaining | Speed: 12.6 samples/sec\n",
      "‚ö° Progress: 1,901/2,949 (64.5%) | Time: 2.5m elapsed, 1.4m remaining | Speed: 12.6 samples/sec\n",
      "‚ö° Progress: 2,001/2,949 (67.9%) | Time: 2.6m elapsed, 1.3m remaining | Speed: 12.6 samples/sec\n",
      "‚ö° Progress: 2,101/2,949 (71.2%) | Time: 2.8m elapsed, 1.1m remaining | Speed: 12.5 samples/sec\n",
      "‚ö° Progress: 2,201/2,949 (74.6%) | Time: 3.0m elapsed, 1.0m remaining | Speed: 12.4 samples/sec\n",
      "‚ö° Progress: 2,301/2,949 (78.0%) | Time: 3.1m elapsed, 52.3s remaining | Speed: 12.4 samples/sec\n",
      "‚ö° Progress: 2,401/2,949 (81.4%) | Time: 3.3m elapsed, 44.7s remaining | Speed: 12.3 samples/sec\n",
      "‚ö° Progress: 2,501/2,949 (84.8%) | Time: 3.4m elapsed, 36.5s remaining | Speed: 12.3 samples/sec\n",
      "‚ö° Progress: 2,601/2,949 (88.2%) | Time: 3.5m elapsed, 28.3s remaining | Speed: 12.3 samples/sec\n",
      "‚ö° Progress: 2,701/2,949 (91.6%) | Time: 3.7m elapsed, 20.2s remaining | Speed: 12.3 samples/sec\n",
      "‚ö° Progress: 2,801/2,949 (95.0%) | Time: 3.8m elapsed, 12.1s remaining | Speed: 12.2 samples/sec\n",
      "‚ö° Progress: 2,901/2,949 (98.4%) | Time: 4.0m elapsed, 3.9s remaining | Speed: 12.2 samples/sec\n",
      "\n",
      "üéØ === OPTIMIZED EXTRACTION COMPLETED ===\n",
      "‚è±Ô∏è  Total time: 4.0 minutes (241.8 seconds)\n",
      "‚ö° Average speed: 12.2 samples/second\n",
      "\n",
      "üìä AUDIO: 2,949 success, 0 errors (100.0% success)\n",
      "üìä VIDEO: 2,949 success, 0 errors (100.0% success)\n",
      "üìä TEXT: 2,949 success, 0 errors (100.0% success)\n",
      "\n",
      "üìä Step 3: Validating extraction results...\n",
      "   üéµ Audio embeddings: 12,784\n",
      "   üé¨ Video embeddings: 12,779\n",
      "   üìù Text embeddings: 12,779\n",
      "\n",
      "üîó Step 4: Creating fusion manifest...\n",
      "   Synchronized samples (all 3 modalities): 12,779\n",
      "   ‚úÖ Processed 2,001 fusion samples...\n",
      "   ‚úÖ Processed 2,949 fusion samples...\n",
      "   ‚úÖ Processed 2,949 fusion samples...\n",
      "   ‚úÖ Processed 2,949 fusion samples...\n",
      "   ‚úÖ Processed 2,949 fusion samples...\n",
      "   ‚úÖ Processed 2,949 fusion samples...\n",
      "\n",
      "üìÑ Fusion manifest created: artifacts/fusion_manifest_20k.csv\n",
      "   üìä Total samples: 2,949\n",
      "   üìä Unique videos: 24\n",
      "   üìä Unique labels: 8\n",
      "\n",
      "üìà Video distribution (top 10):\n",
      "    1. Video 802: 273 samples\n",
      "    2. Video 504: 209 samples\n",
      "    3. Video 18: 183 samples\n",
      "    4. Video 242: 177 samples\n",
      "    5. Video 257: 168 samples\n",
      "    6. Video 439: 168 samples\n",
      "    7. Video 145: 166 samples\n",
      "    8. Video 235: 162 samples\n",
      "    9. Video 378: 160 samples\n",
      "   10. Video 112: 146 samples\n",
      "\n",
      "üéâ OPTIMIZED EXTRACTION PIPELINE COMPLETED SUCCESSFULLY!\n",
      "‚úÖ Results: 2,949 embeddings from 24 videos\n",
      "üïê Completed at: 2025-09-20 13:16:33\n",
      "‚úÖ Ready for fusion model training!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ EXECUTE OPTIMIZED EMBEDDING EXTRACTION & CREATE FUSION MANIFEST\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéØ STARTING OPTIMIZED EMBEDDING EXTRACTION PIPELINE\")\n",
    "print(f\"üïê Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Step 1: Create directories\n",
    "print(\"\\nüìÅ Step 1: Creating directories...\")\n",
    "os.makedirs(\"specialists/audio\", exist_ok=True)\n",
    "os.makedirs(\"specialists/video\", exist_ok=True) \n",
    "os.makedirs(\"specialists/text\", exist_ok=True)\n",
    "print(\"‚úÖ Directories created\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Step 2: Execute the OPTIMIZED extraction function\n",
    "print(\"\\nüöÄ Step 2: Extracting embeddings with optimized pipeline...\")\n",
    "print(\"‚ö° This processes all modalities together for maximum efficiency\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Run the OPTIMIZED extraction\n",
    "embedding_results_final = extract_all_embeddings_optimized(max_samples=20000, save_embeddings=True)\n",
    "\n",
    "# Step 3: Check extraction results\n",
    "print(\"\\nüìä Step 3: Validating extraction results...\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "try:\n",
    "    audio_count = len([f for f in os.listdir(\"specialists/audio\") if f.endswith('.npy')])\n",
    "    video_count = len([f for f in os.listdir(\"specialists/video\") if f.endswith('.npy')])\n",
    "    text_count = len([f for f in os.listdir(\"specialists/text\") if f.endswith('.npy')])\n",
    "    \n",
    "    print(f\"   üéµ Audio embeddings: {audio_count:,}\")\n",
    "    print(f\"   üé¨ Video embeddings: {video_count:,}\")\n",
    "    print(f\"   üìù Text embeddings: {text_count:,}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Step 4: Create synchronized fusion manifest (only if we have embeddings)\n",
    "    if audio_count > 0 or video_count > 0 or text_count > 0:\n",
    "        print(\"\\nüîó Step 4: Creating fusion manifest...\")\n",
    "        min_count = min(audio_count, video_count, text_count)\n",
    "        print(f\"   Synchronized samples (all 3 modalities): {min_count:,}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        fusion_data = []\n",
    "        processed_count = 0\n",
    "        \n",
    "        for i in range(min_count):\n",
    "            try:\n",
    "                # Verify all embedding files exist\n",
    "                audio_path = f\"specialists/audio/audio_embedding_{i:06d}.npy\"\n",
    "                video_path = f\"specialists/video/video_embedding_{i:06d}.npy\"\n",
    "                text_path = f\"specialists/text/text_embedding_{i:06d}.npy\"\n",
    "                \n",
    "                if os.path.exists(audio_path) and os.path.exists(video_path) and os.path.exists(text_path):\n",
    "                    # Get metadata from original dataset\n",
    "                    if i < len(datasets['audio']):\n",
    "                        _, label, video_id_raw = datasets['audio'][i]\n",
    "                        \n",
    "                        # Handle video_id format\n",
    "                        if isinstance(video_id_raw, dict):\n",
    "                            video_id = video_id_raw.get('video_id', i)\n",
    "                        else:\n",
    "                            video_id = video_id_raw\n",
    "                        \n",
    "                        fusion_data.append({\n",
    "                            'audio_path': audio_path,\n",
    "                            'video_path': video_path,\n",
    "                            'text_path': text_path,\n",
    "                            'label': int(label),\n",
    "                            'video_id': str(video_id),\n",
    "                            'sample_idx': i\n",
    "                        })\n",
    "                        processed_count += 1\n",
    "                        \n",
    "                if i % 2000 == 0 and i > 0:\n",
    "                    print(f\"   ‚úÖ Processed {processed_count:,} fusion samples...\")\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error processing sample {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create and save fusion manifest\n",
    "        if fusion_data:\n",
    "            fusion_df = pd.DataFrame(fusion_data)\n",
    "            fusion_manifest_path = \"artifacts/fusion_manifest_20k.csv\"\n",
    "            fusion_df.to_csv(fusion_manifest_path, index=False)\n",
    "            \n",
    "            print(f\"\\nüìÑ Fusion manifest created: {fusion_manifest_path}\")\n",
    "            print(f\"   üìä Total samples: {len(fusion_df):,}\")\n",
    "            print(f\"   üìä Unique videos: {fusion_df['video_id'].nunique()}\")\n",
    "            print(f\"   üìä Unique labels: {fusion_df['label'].nunique()}\")\n",
    "            \n",
    "            # Display video distribution\n",
    "            print(f\"\\nüìà Video distribution (top 10):\")\n",
    "            video_counts = fusion_df['video_id'].value_counts()\n",
    "            for i, (video_id, count) in enumerate(video_counts.head(10).items()):\n",
    "                print(f\"   {i+1:2d}. Video {video_id}: {count:,} samples\")\n",
    "            \n",
    "            print(f\"\\nüéâ OPTIMIZED EXTRACTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"‚úÖ Results: {len(fusion_df):,} embeddings from {fusion_df['video_id'].nunique()} videos\")\n",
    "            print(f\"üïê Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"‚úÖ Ready for fusion model training!\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid fusion samples created!\")\n",
    "    else:\n",
    "        print(\"‚ùå No embeddings were created!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in validation step: {e}\")\n",
    "\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b93e6",
   "metadata": {},
   "source": [
    "## üéØ Summary & Usage Guide\n",
    "\n",
    "### üìÅ **Generated Files:**\n",
    "After running this notebook, you'll have:\n",
    "\n",
    "- **`artifacts/embeddings/audio/`** - Audio embeddings (.npy files)\n",
    "- **`artifacts/embeddings/video/`** - Video embeddings (.npy files)  \n",
    "- **`artifacts/embeddings/text/`** - Text embeddings (.npy files)\n",
    "- **`artifacts/embeddings/fusion_manifest.csv`** - Master manifest for fusion training\n",
    "- **`artifacts/embeddings/sample_fusion_vector.npy`** - Sample concatenated vector\n",
    "\n",
    "### üöÄ **How to Use for Fusion Training:**\n",
    "\n",
    "```python\n",
    "# 1. Load the fusion manifest\n",
    "fusion_df = pd.read_csv(\"artifacts/embeddings/fusion_manifest.csv\")\n",
    "\n",
    "# 2. For each sample, load and concatenate embeddings\n",
    "def load_fusion_sample(row):\n",
    "    embeddings = []\n",
    "    \n",
    "    # Load audio embedding\n",
    "    if 'embedding_path_audio' in row:\n",
    "        audio_emb = np.load(row['embedding_path_audio'])\n",
    "        embeddings.append(audio_emb)\n",
    "    \n",
    "    # Load video embedding  \n",
    "    if 'embedding_path_video' in row:\n",
    "        video_emb = np.load(row['embedding_path_video'])\n",
    "        embeddings.append(video_emb)\n",
    "        \n",
    "    # Load text embedding\n",
    "    if 'embedding_path_text' in row:\n",
    "        text_emb = np.load(row['embedding_path_text'])\n",
    "        embeddings.append(text_emb)\n",
    "    \n",
    "    # Concatenate all modalities\n",
    "    fusion_vector = np.concatenate(embeddings)\n",
    "    return fusion_vector, row['label']\n",
    "\n",
    "# 3. Use in your fusion model training\n",
    "for _, row in fusion_df.iterrows():\n",
    "    features, label = load_fusion_sample(row)\n",
    "    # Train your fusion model with these features\n",
    "```\n",
    "\n",
    "### ‚ö° **Configuration Options:**\n",
    "- **Batch Size**: Modify `config['batch_size']` in cell 1\n",
    "- **Sample Limit**: Adjust `max_samples` in cell 5 \n",
    "- **Output Directory**: Change `config['output_dir']` in cell 1\n",
    "\n",
    "### üîß **Troubleshooting:**\n",
    "- **\"Model not found\"**: Check if your specialist models are trained and saved\n",
    "- **\"Dataset empty\"**: Verify your manifest CSV files exist and have data\n",
    "- **\"Out of memory\"**: Reduce batch_size or max_samples\n",
    "\n",
    "### üìä **Performance Tips:**\n",
    "- Start with 1000 samples for testing\n",
    "- Use GPU for faster extraction\n",
    "- Process in smaller batches if memory limited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vitfix)",
   "language": "python",
   "name": "vitfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
