{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e4204e",
   "metadata": {},
   "source": [
    "# 🎯 Text Specialist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd4ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 8 emotion classes: ['Anger', 'Fear', 'Joy', 'Neutral', 'Proud', 'Sadness', 'Surprise', 'Trust']\n"
     ]
    }
   ],
   "source": [
    "# 📋 CELL 1: Setup & Imports (⏱️ ~10 seconds)\n",
    "# Enhanced multilingual emotion prediction pipeline for Whisper-transcribed text\n",
    "\n",
    "# Core imports\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Scientific computing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Transformers and ML\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load label mapping\n",
    "with open(\"artifacts/label2idx.json\", \"r\") as f:\n",
    "    label2idx = json.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(label2idx)} emotion classes: {list(label2idx.keys())}\")\n",
    "num_classes = len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b80dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 GPU SETUP\n",
      "========================================\n",
      "✅ CUDA Available: True\n",
      "🎯 Device: cuda\n",
      "🔧 GPU: NVIDIA GeForce RTX 4070\n",
      "💾 Memory: 12.0 GB\n",
      "🧪 Testing GPU...\n",
      "✅ GPU test: Success\n",
      "✅ Mixed precision available\n",
      "⚡ Entry GPU: batch_size=12 recommended\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 🚀 CELL 2: GPU Setup & Optimization (⏱️ ~5 seconds)\n",
    "print(\"🔥 GPU SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"✅ CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎯 Device: {device}\")\n",
    "    print(f\"🔧 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"💾 Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Quick GPU test (reduced size for faster execution)\n",
    "    print(\"🧪 Testing GPU...\")\n",
    "    test_tensor = torch.randn(100, 100).to(device)  # Reduced from 1000x1000 to 100x100\n",
    "    result = torch.mm(test_tensor, test_tensor)\n",
    "    print(f\"✅ GPU test: Success\")\n",
    "    \n",
    "    # Optimization settings\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Mixed precision check\n",
    "    try:\n",
    "        use_amp = True\n",
    "        print(\"✅ Mixed precision available\")\n",
    "    except ImportError:\n",
    "        use_amp = False\n",
    "        print(\"⚠️  Mixed precision not available\")\n",
    "    \n",
    "    # Recommended batch size based on GPU memory\n",
    "    if gpu_memory >= 24:\n",
    "        recommended_batch = 32\n",
    "        print(f\"🔥 High-end GPU: batch_size=32 recommended\")\n",
    "    elif gpu_memory >= 12:\n",
    "        recommended_batch = 16\n",
    "        print(f\"🚀 Mid-range GPU: batch_size=16 recommended\")\n",
    "    elif gpu_memory >= 8:\n",
    "        recommended_batch = 12\n",
    "        print(f\"⚡ Entry GPU: batch_size=12 recommended\")\n",
    "    else:\n",
    "        recommended_batch = 4\n",
    "        print(f\"🤏 Low memory GPU: batch_size=4 recommended\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ CUDA not available - using CPU\")\n",
    "    print(\"⚠️  Training will be slow on CPU\")\n",
    "    recommended_batch = 8\n",
    "    use_amp = False\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4363b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training function ready!\n"
     ]
    }
   ],
   "source": [
    "# 🏋️ CELL 4: Simple Fast Training Function (⏱️ ~1 second)\n",
    "def train_specialist_verbose(model, train_loader, val_loader, num_epochs=5, lr=1e-5, use_temporal=True, use_mixed_precision=True):\n",
    "    \"\"\"Lightweight training function - instant definition\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scaler = GradScaler() if use_mixed_precision and device.type == 'cuda' else None\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    training_history = []\n",
    "    \n",
    "    print(\"🚀 TRAINING STARTED\")\n",
    "    print(f\"📊 {len(train_loader.dataset):,} samples, {len(train_loader)} batches\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n🔥 Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (tokens, y, metadata) in enumerate(train_loader):\n",
    "            input_ids = tokens['input_ids'].to(device)\n",
    "            attention_mask = tokens['attention_mask'].to(device)\n",
    "            y = y.to(device)\n",
    "            temporal_features = metadata['temporal_features'].to(device) if use_temporal else None\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if scaler:\n",
    "                with autocast():\n",
    "                    logits = model(input_ids=input_ids, attention_mask=attention_mask, temporal_features=temporal_features)\n",
    "                    loss = criterion(logits, y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, temporal_features=temporal_features)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pred = logits.argmax(dim=1)\n",
    "            train_correct += (pred == y).sum().item()\n",
    "            train_total += y.size(0)\n",
    "            \n",
    "            # Progress every 20 batches\n",
    "            if batch_idx % 20 == 0:\n",
    "                samples_done = min((batch_idx + 1) * train_loader.batch_size, len(train_loader.dataset))\n",
    "                progress = (batch_idx + 1) / len(train_loader) * 100\n",
    "                acc = (train_correct / train_total) * 100 if train_total > 0 else 0\n",
    "                \n",
    "                print(f\"  📈 {samples_done:,} samples ({progress:.1f}%) | Loss: {loss.item():.3f} | Acc: {acc:.1f}%\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tokens, y, metadata in val_loader:\n",
    "                input_ids = tokens['input_ids'].to(device)\n",
    "                attention_mask = tokens['attention_mask'].to(device)\n",
    "                y = y.to(device)\n",
    "                temporal_features = metadata['temporal_features'].to(device) if use_temporal else None\n",
    "                \n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, temporal_features=temporal_features)\n",
    "                loss = criterion(logits, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                pred = logits.argmax(dim=1)\n",
    "                val_correct += (pred == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "                \n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "        \n",
    "        # Results\n",
    "        train_acc = (train_correct / train_total) * 100\n",
    "        val_acc = (val_correct / val_total) * 100\n",
    "        f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f\"✅ Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | F1: {f1_macro:.3f} | Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        if f1_macro > best_val_f1:\n",
    "            best_val_f1 = f1_macro\n",
    "            print(f\"🏆 NEW BEST F1: {best_val_f1:.3f}\")\n",
    "        \n",
    "        training_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss / len(train_loader),\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss / len(val_loader),\n",
    "            'val_acc': val_acc,\n",
    "            'f1_macro': f1_macro,\n",
    "            'epoch_time': epoch_time\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n🏆 DONE! Best F1: {best_val_f1:.3f}\")\n",
    "    return model, training_history\n",
    "\n",
    "print(\"✅ Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c009f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WhisperTextPreprocessor class ready\n"
     ]
    }
   ],
   "source": [
    "# 🧹 CELL 3: Text Preprocessor for Whisper (⏱️ ~2 seconds)\n",
    "class WhisperTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Specialized preprocessing for Whisper-transcribed text\n",
    "    Handles common transcription artifacts and multilingual content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common Whisper artifacts to clean\n",
    "        self.whisper_artifacts = [\n",
    "            r'\\[.*?\\]',  # Remove [MUSIC], [NOISE] etc.\n",
    "            r'\\(.*?\\)',  # Remove (unintelligible) etc.\n",
    "            r'\\.{3,}',   # Remove excessive dots\n",
    "            r'-{2,}',    # Remove excessive dashes\n",
    "            r'\\s{2,}',   # Multiple spaces to single space\n",
    "        ]\n",
    "        \n",
    "        # Indonesian filler words and false starts\n",
    "        self.filler_words = ['eh', 'em', 'uh', 'um', 'ya', 'gitu', 'jadi']\n",
    "        \n",
    "    def clean_whisper_text(self, text: str) -> str:\n",
    "        \"\"\"Clean common Whisper transcription artifacts\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return \"\"\n",
    "            \n",
    "        text = str(text).lower().strip()\n",
    "        \n",
    "        # Remove Whisper artifacts\n",
    "        for pattern in self.whisper_artifacts:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove numbers (often transcription errors)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_temporal_features(self, text: str, start_time: float, end_time: float) -> Dict:\n",
    "        \"\"\"Extract features that might correlate with temporal position\"\"\"\n",
    "        duration = end_time - start_time\n",
    "        word_count = len(text.split()) if text else 0\n",
    "        speech_rate = word_count / duration if duration > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'duration': duration,\n",
    "            'word_count': word_count, \n",
    "            'speech_rate': speech_rate,\n",
    "            'start_time_norm': start_time,  # Can normalize later\n",
    "            'text_length': len(text)\n",
    "        }\n",
    "\n",
    "print(\"✅ WhisperTextPreprocessor class ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3dcf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ EnhancedTextDataset class ready\n"
     ]
    }
   ],
   "source": [
    "# 📊 CELL 4: Enhanced Dataset with Temporal Features (⏱️ ~3 seconds)\n",
    "class EnhancedTextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Enhanced dataset for Whisper-transcribed text with temporal features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, manifest_csv, split=\"train\", fold=0, \n",
    "                 model_name=\"bert-base-multilingual-cased\", max_len=128, \n",
    "                 label2idx=None, use_temporal_features=True,\n",
    "                 apply_text_cleaning=True):\n",
    "        \n",
    "        df = pd.read_csv(manifest_csv)\n",
    "        \n",
    "        # Filter by split and fold\n",
    "        if split == \"train\":\n",
    "            self.data = df[df['fold'] != fold].reset_index(drop=True)\n",
    "        else:\n",
    "            self.data = df[df['fold'] == fold].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"📊 {split.title()} split (fold {fold}): {len(self.data)} samples\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label2idx = label2idx or {}\n",
    "        self.use_temporal_features = use_temporal_features\n",
    "        self.preprocessor = WhisperTextPreprocessor() if apply_text_cleaning else None\n",
    "        \n",
    "        # Preprocess text data\n",
    "        if self.preprocessor:\n",
    "            print(\"🧹 Cleaning Whisper text...\")\n",
    "            self.data['text_snippet'] = self.data['text_snippet'].apply(\n",
    "                self.preprocessor.clean_whisper_text\n",
    "            )\n",
    "        \n",
    "        # Extract temporal features if enabled\n",
    "        if self.use_temporal_features:\n",
    "            print(\"⏰ Extracting temporal features...\")\n",
    "            temporal_features = []\n",
    "            for _, row in self.data.iterrows():\n",
    "                features = self.preprocessor.extract_temporal_features(\n",
    "                    row['text_snippet'], row['start'], row['end']\n",
    "                ) if self.preprocessor else {\n",
    "                    'duration': row['end'] - row['start'],\n",
    "                    'word_count': len(str(row['text_snippet']).split()),\n",
    "                    'speech_rate': 0,\n",
    "                    'start_time_norm': row['start'],\n",
    "                    'text_length': len(str(row['text_snippet']))\n",
    "                }\n",
    "                temporal_features.append(features)\n",
    "            \n",
    "            # Add temporal features to dataframe\n",
    "            temporal_df = pd.DataFrame(temporal_features)\n",
    "            for col in temporal_df.columns:\n",
    "                self.data[f'temp_{col}'] = temporal_df[col]\n",
    "        \n",
    "        print(f\"✅ Dataset ready: {len(self.data)} samples with temporal features\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['text_snippet'])\n",
    "        label = self.label2idx[row['label']]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Prepare temporal features\n",
    "        temporal_features = []\n",
    "        if self.use_temporal_features:\n",
    "            temporal_cols = ['temp_duration', 'temp_word_count', 'temp_speech_rate', \n",
    "                           'temp_start_time_norm', 'temp_text_length']\n",
    "            temporal_features = [float(row[col]) for col in temporal_cols if col in row]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'temporal_features': torch.tensor(temporal_features, dtype=torch.float),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✅ EnhancedTextDataset class ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9744bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Initializing Ultra-Lightweight Emotion Classifier...\n",
      "🚀 Loading fast model: prajjwal1/bert-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fast model ready with 11,173,064 parameters\n",
      "✅ Model ready on cuda\n"
     ]
    }
   ],
   "source": [
    "# 🧠 CELL 6: Fast Model Architecture (⏱️ ~3 seconds)\n",
    "class FastEmotionClassifier(nn.Module):\n",
    "    \"\"\"Fast emotion classifier for quick testing\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"prajjwal1/bert-mini\", num_classes=8, \n",
    "                 use_temporal=True, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use DistilBERT for faster loading and inference\n",
    "        print(f\"🚀 Loading fast model: {model_name}\")\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.model_name = model_name\n",
    "        self.use_temporal = use_temporal\n",
    "        \n",
    "        # Get hidden size\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Simple temporal processing\n",
    "        if use_temporal:\n",
    "            self.temporal_dim = 5\n",
    "            self.temporal_processor = nn.Linear(self.temporal_dim, 32)\n",
    "            combined_size = hidden_size + 32\n",
    "        else:\n",
    "            combined_size = hidden_size\n",
    "        \n",
    "        # Simple classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(combined_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Fast model ready with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, temporal_features=None):\n",
    "        # BERT encoding\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Add temporal features if available\n",
    "        if self.use_temporal and temporal_features is not None:\n",
    "            temporal_processed = self.temporal_processor(temporal_features)\n",
    "            combined_features = torch.cat([cls_output, temporal_processed], dim=1)\n",
    "        else:\n",
    "            combined_features = cls_output\n",
    "        \n",
    "        return self.classifier(combined_features)\n",
    "\n",
    "# Initialize ultra-lightweight model\n",
    "print(\"🧠 Initializing Ultra-Lightweight Emotion Classifier...\")\n",
    "model = FastEmotionClassifier(\n",
    "    model_name=\"prajjwal1/bert-mini\",  # Ultra-lightweight: only 11M parameters!\n",
    "    num_classes=num_classes,\n",
    "    use_temporal=True,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"✅ Model ready on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e50eab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 FAST DATASET LOADING\n",
      "========================================\n",
      "📂 Using: artifacts/train_manifest_fold0.csv\n",
      "📊 Dataset: 4,271 samples\n",
      "🎯 Using ALL available data (already preprocessed/reduced)\n",
      "📊 Full dataset: 3,416 train, 855 validation\n",
      "📦 Creating full datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilkom\\miniconda3\\envs\\vitheoa\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Optimized batch size: 12 for 3,416 samples\n",
      "✅ Full datasets ready: 3,416 train, 855 val\n",
      "🚀 Batch size: 12 | Mode: FULL\n",
      "💡 Using ALL available data - ready for full training!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 CELL 7: Fast Dataset Loading (⏱️ ~5-10 seconds)\n",
    "print(\"🔧 FAST DATASET LOADING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Quick file discovery\n",
    "artifacts_path = \"artifacts\" \n",
    "available_files = [f for f in os.listdir(artifacts_path) if f.endswith('.csv')]\n",
    "\n",
    "# Find dataset file quickly\n",
    "manifest_file = None\n",
    "for preferred in ['train_manifest_fold0.csv', 'train_manifest_all.csv']:\n",
    "    if preferred in available_files:\n",
    "        manifest_file = f\"artifacts/{preferred}\"\n",
    "        break\n",
    "\n",
    "if not manifest_file:\n",
    "    manifest_file = f\"artifacts/{available_files[0]}\"  # Use first available\n",
    "\n",
    "print(f\"📂 Using: {manifest_file}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(manifest_file)\n",
    "print(f\"📊 Dataset: {df.shape[0]:,} samples\")\n",
    "\n",
    "# Add timing columns if missing\n",
    "if not all(col in df.columns for col in ['start', 'end']):\n",
    "    df['start'] = 0.0\n",
    "    df['end'] = 1.0\n",
    "\n",
    "# Quick train/val split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# FULL MODE - Use ALL available data (since data was already reduced in preprocessing)\n",
    "TRAINING_MODE = \"FULL\"\n",
    "print(f\"🎯 Using ALL available data (already preprocessed/reduced)\")\n",
    "print(f\"📊 Full dataset: {len(train_df):,} train, {len(val_df):,} validation\")\n",
    "\n",
    "# Save splits\n",
    "train_df.to_csv(\"artifacts/temp_train_full.csv\", index=False)\n",
    "val_df.to_csv(\"artifacts/temp_val_full.csv\", index=False)\n",
    "\n",
    "# Simple collation function\n",
    "def collate_fn_simple(batch):\n",
    "    tokens = {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch])\n",
    "    }\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    temporal_features = torch.stack([item['temporal_features'] for item in batch])\n",
    "    metadata = {'temporal_features': temporal_features}\n",
    "    return tokens, labels, metadata\n",
    "\n",
    "# Fast dataset class (minimal processing)\n",
    "class FastTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, label2idx, model_name, max_len=128):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.label2idx = label2idx\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['text_snippet'])[:200]  # Limit text length for speed\n",
    "        label = self.label2idx[row['label']]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Simple temporal features (no complex computation)\n",
    "        temporal_features = torch.tensor([\n",
    "            row.get('end', 1.0) - row.get('start', 0.0),  # duration\n",
    "            len(text.split()),  # word count\n",
    "            0.5,  # dummy speech rate\n",
    "            row.get('start', 0.0),  # start time\n",
    "            len(text)  # text length\n",
    "        ], dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'temporal_features': temporal_features\n",
    "        }\n",
    "\n",
    "print(\"📦 Creating full datasets...\")\n",
    "\n",
    "# Create full datasets using ALL available data\n",
    "train_dataset = FastTextDataset(\"artifacts/temp_train_full.csv\", label2idx, model.model_name)\n",
    "val_dataset = FastTextDataset(\"artifacts/temp_val_full.csv\", label2idx, model.model_name)\n",
    "\n",
    "# Optimize batch size based on dataset size and GPU memory\n",
    "dataset_size = len(train_dataset)\n",
    "if dataset_size > 10000:\n",
    "    batch_size = recommended_batch  # Use GPU-optimized batch size from Cell 2\n",
    "elif dataset_size > 5000:\n",
    "    batch_size = 16\n",
    "else:\n",
    "    batch_size = 12\n",
    "\n",
    "print(f\"🚀 Optimized batch size: {batch_size} for {dataset_size:,} samples\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_simple,\n",
    "    num_workers=2,  # Enable workers for faster loading\n",
    "    pin_memory=True  # Enable for better GPU performance\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_simple,\n",
    "    num_workers=2,  # Enable workers for faster loading\n",
    "    pin_memory=True  # Enable for better GPU performance\n",
    ")\n",
    "\n",
    "# Store info\n",
    "train_loader.training_mode = TRAINING_MODE\n",
    "train_loader.estimated_time = \"15-45 minutes (depending on dataset size)\"\n",
    "\n",
    "print(f\"✅ Full datasets ready: {len(train_dataset):,} train, {len(val_dataset):,} val\")\n",
    "print(f\"🚀 Batch size: {batch_size} | Mode: {TRAINING_MODE}\")\n",
    "print(f\"💡 Using ALL available data - ready for full training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f77aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaders found!\n",
      "📊 Training: 3,416 | Validation: 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilkom\\AppData\\Local\\Temp\\ipykernel_91864\\3898306881.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if use_mixed_precision and device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TRAINING STARTED\n",
      "📊 3,416 samples, 285 batches\n",
      "\n",
      "🔥 Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# 🏋️ CELL 8: Start Training (⏱️ ~15-45 minutes)\n",
    "# IMPORTANT: Run Cell 7 (Dataset Loading) first\n",
    "\n",
    "# Check if data loaders exist\n",
    "try:\n",
    "    _ = train_loader\n",
    "    _ = val_loader\n",
    "    print(\"✅ Data loaders found!\")\n",
    "    print(f\"📊 Training: {len(train_loader.dataset):,} | Validation: {len(val_loader.dataset):,}\")\n",
    "except NameError:\n",
    "    print(\"❌ Error: train_loader and val_loader not found!\")\n",
    "    print(\"💡 Please run Cell 7 first to load the datasets\")\n",
    "\n",
    "# Start training with optimized parameters for FULL mode\n",
    "trained_model, training_history = train_specialist_verbose(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=5,  # Full training epochs\n",
    "    lr=2e-5,       # Optimized learning rate for full dataset\n",
    "    use_temporal=True,\n",
    "    use_mixed_precision=use_amp\n",
    ")\n",
    "\n",
    "print(\"🎯 Training completed! Check training_history for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 CELL 9: Save Trained Model (⏱️ ~5 seconds)\n",
    "def save_trained_model():\n",
    "    \"\"\"Save the trained model and training history\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if trained model exists\n",
    "        _ = trained_model\n",
    "        _ = training_history\n",
    "        print(\"✅ Trained model found!\")\n",
    "    except NameError:\n",
    "        print(\"❌ Error: trained_model not found!\")\n",
    "        print(\"💡 Please run Cell 8 first to train the model\")\n",
    "        return False\n",
    "    \n",
    "    print(\"💾 Saving Trained Model...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = \"checkpoints/text_specialist_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': trained_model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_name': trained_model.model_name,\n",
    "            'num_classes': num_classes,\n",
    "            'use_temporal': True,\n",
    "            'dropout_rate': 0.3\n",
    "        },\n",
    "        'label2idx': label2idx,\n",
    "        'training_history': training_history\n",
    "    }, model_path)\n",
    "    \n",
    "    print(f\"✅ Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = \"checkpoints/training_history.json\"\n",
    "    import json\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    if training_history:\n",
    "        best_f1 = max(epoch['f1_macro'] for epoch in training_history)\n",
    "        final_f1 = training_history[-1]['f1_macro']\n",
    "        print(f\"\\n🏆 Training Summary:\")\n",
    "        print(f\"   🎯 Best F1 Score: {best_f1:.4f}\")\n",
    "        print(f\"   📊 Final F1 Score: {final_f1:.4f}\")\n",
    "        print(f\"   📈 Epochs trained: {len(training_history)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Uncomment the line below to save the trained model\n",
    "# save_trained_model()\n",
    "print(\"💡 Uncomment the last line above to save trained model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vitfix)",
   "language": "python",
   "name": "vitfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
